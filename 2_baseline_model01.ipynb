{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Low resolution Detection of flooded tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this model is used to train model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import src.baseline_model01 as bm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing\n",
    "\n",
    "The constructor handle a small amount of data processing :\n",
    "\n",
    "* Pre-processing of the static data \n",
    "* Construction of the labels from raw data\n",
    "* Loading of the dynamic data\n",
    "\n",
    "And the splitting of the dataset into test / train / validation set follonwing defined start date and end date. The validation set could be useful as an alternative for Cross Validation to find optimal hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator = bm.BaseLineModel(\n",
    "    \"localdata/final_label_Full_ERA5.nc\",\n",
    "    dynamic_features_path = \"localdata/raw/ERA5_train.nc\",\n",
    "    static_features_path = \"localdata/static_ERA5.nc\",\n",
    "    train_start = \"2002-08-03\", # date where to split train test\n",
    "    train_end = \"2003-01-1\", # date where to split train test\n",
    "    test_start = \"2003-01-01\", # date where to split train test\n",
    "    test_end = \"2003-03-17\", # date where to split train test\n",
    "    name = \"Model_01_default\",\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The baseline model used here is a random forest (Random forest is used as an exemple of a basic ML approach sometime used for flood prediction [<a id=\"1\" href=\"https://www.tandfonline.com/doi/full/10.1080/19475705.2017.1308971\">1</a>,<a id=\"2\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S0022169415004217\">2</a>,<a id=\"3\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S004896971934971X\">3</a>]), it is by no way the only approach for such endeavor.\n",
    "\n",
    "To train the model the following parameters must be defined :\n",
    "\n",
    "* selected static features (geospatial data)\n",
    "* selected dynamic features (climate data):\n",
    "   * Depth of history to process climate features\n",
    "   * Usage of climate feature derivative  \n",
    "\n",
    "\n",
    "## References\n",
    "<a id=\"1\" href=\"https://www.tandfonline.com/doi/full/10.1080/19475705.2017.1308971\">[1]</a> \n",
    "Lee, S., Kim, J. C., Jung, H. S., Lee, M. J., & Lee, S. (2017). Spatial prediction of flood susceptibility using random-forest and boosted-tree models in Seoul metropolitan city, Korea. Geomatics, Natural Hazards and Risk, 8(2), 1185-1203.\n",
    "\n",
    "<a id=\"2\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S0022169415004217\">[2]</a>\n",
    "Wang, Z., Lai, C., Chen, X., Yang, B., Zhao, S., & Bai, X. (2015). Flood hazard risk assessment model based on random forest. Journal of Hydrology, 527, 1130-1141.\n",
    "\n",
    "<a id=\"3\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S004896971934971X\">[3]</a>\n",
    "Chen, W., Li, Y., Xue, W., Shahabi, H., Li, S., Hong, H., ... & Ahmad, B. B. (2020). Modeling flood susceptibility using data-driven approaches of na√Øve bayes tree, alternating decision tree, and random forest methods. Science of The Total Environment, 701, 134979.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training parameters bellow are the best one we found during the genetic algorithm optimisation (see 'model optimisation' section):\n",
    "\n",
    "#model, acc = baseline_model_generator.load_indiv([True, True, 1, True, True, 0, True, True, True, True, 1, False, True, True, False, True, 0, 289, 10, 37, 14], False)\n",
    "model, acc = baseline_model_generator.train_model([False, # 'soilgrid_bdod'\n",
    "                                                  True, # 'soilgrid_cfvo'\n",
    "                                                  False, # 'soilgrid_silt'\n",
    "                                                  True, # 'soilgrid_clay'\n",
    "                                                  False, # 'soilgrid_sand' \n",
    "                                                  False, # 'depth_to_bedrock'\n",
    "                                                  True, # 'altitude'\n",
    "                                                  False, #  'aspect'\n",
    "                                                  True, # 'slope'\n",
    "                                                  False, # 'water_density'\n",
    "                                                  True, # 'watershed'\n",
    "                                                  False, # 'topological_catchment_areas'\n",
    "                                                  True, # 'dist_sea'\n",
    "                                                  True, # 'dist_riv'\n",
    "                                                  True, # 'tp'\n",
    "                                                  True, # 't2m'\n",
    "                                                  False, # 'use deriv'\n",
    "                                                  195, # nb. Trees\n",
    "                                                  8, # Trees dept\n",
    "                                                  83, # Hist dept 1\n",
    "                                                  11 # Hist dept 2\n",
    "                                                  ], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model\n",
    "\n",
    "This method save the BaselineModel object, by default the *name* atribute is used as the file name (if you save a new model with the same name it will replace the previous one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.save_to_disk(name=\"Model_01_default\")\n",
    "\n",
    "#if you want to load a previously trained model :\n",
    "#baseline_model_generator = baseline_model_generator.load_from_disk(name=\"Model_01_default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model optimisation\n",
    "\n",
    "We chose here to look for good hyper-parameters using a Genetic Algorithm (GA), exemple of alternatives to GA for hyper parameters optimisation includes : random search, grid search...\n",
    "\n",
    "To use the GA opimiser method you need to define a population size and a number of generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_model_generator.GA_optimisation(ngen = 40, pop = 80, best_individuals = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "\n",
    "We propose a few helper methods for model analysis, such as :\n",
    "\n",
    "* Feature importance\n",
    "* Graph of metrics such as ROC, AP\n",
    "* Results mapping\n",
    "\n",
    "\n",
    "#### Feature importance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.compute_all_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.print_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve\n",
    "\n",
    "auc_graph accept a data set name (train / test / val), a metric by default it will output results for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.auc_graph(dataset=[\"Train\",\"Test\"],metrics=\"\", key_thresholds=[0.01,0.1,0.15, 0.2,0.3, 0.5, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction grid\n",
    "\n",
    "To display results we first need to compute the prediction probability for the full data cube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.compute_full_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.save_prediction_map(save_path = \"graph/model1/predictions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_model_generator.save_error_map(save_path=\"graph/model1/baseline_model_generator/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences between labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.save_prediction_map_and_labels(save_path =  \"graph/model1/label_and_pred/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification map at all threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.save_FP_FN_map(save_path = \"graph/model1/FP_FN/\", thresholds = [0.3, 0.5, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data computation for the second model\n",
    "\n",
    "We choose to use the M1 model score (M1_score) as a dynamic feature for the M2 model. \n",
    "The following part create two Xarrays with M1 model score at Full Resolution and ERA5 resolution.\n",
    "\n",
    "\n",
    "## Training Data\n",
    "#### Creation of the M1 score Xarray at ERA5 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_generator.compute_full_grid()\n",
    "Full_Rez = xr.open_dataset(\"localdata/final_label_Full_Rez.nc\")\n",
    "print(np.unique(baseline_model_generator.full_grid_all))\n",
    "xr_array_score = xr.DataArray(baseline_model_generator.full_grid_all, \n",
    "                              dims=[\"time\", \"y\", \"x\"],\n",
    "                              coords={\"time\": Full_Rez.time, \n",
    "                                      \"x\": baseline_model_generator.labels.x, \n",
    "                                      \"y\": baseline_model_generator.labels.y},\n",
    "                              name=\"M1_score\")\n",
    "\n",
    "xr_array_score = xr_array_score.astype('float32')\n",
    "\n",
    "xr_array_score.to_netcdf('localdata/Model1_score_ERA5_Rez_v2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_array_score = xr.open_dataset('localdata/Model1_score_ERA5_Rez_v2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the M1 score Xarray at Full Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_interp = xr_array_score['M1_score'].interp(x=Full_Rez.x, y=Full_Rez.y, method='nearest')\n",
    "Full_Rez = Full_Rez.rename({'__xarray_dataarray_variable__': 'M1_score'})\n",
    "\n",
    "expanded_score = Full_Rez.copy()\n",
    "expanded_score['M1_score'] = small_interp\n",
    "\n",
    "expanded_score['M1_score'] = expanded_score['M1_score'].astype('float32')\n",
    "\n",
    "fill_value = small_interp.mean().item()\n",
    "expanded_score['M1_score'] = expanded_score['M1_score'].fillna(fill_value)\n",
    "expanded_score.to_netcdf('localdata/Model1_Score_Full_Rez_v2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "M1_score should be a float between O and 1. The presence of Nan will make the next model crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_score_ERA5_Rez_v2 = xr.open_dataset('localdata/Model1_score_ERA5_Rez_v2.nc')\n",
    "np.unique(Model1_score_ERA5_Rez_v2.M1_score.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_Score_Full_Rez_v2 = xr.open_dataset('localdata/Model1_Score_Full_Rez_v2.nc')\n",
    "np.unique(Model1_Score_Full_Rez_v2.M1_score.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference / Evaluation Data\n",
    "#### Creation of the M1 score Xarray at ERA5 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slice = baseline_model_generator.labels.sel(time=slice('2003-11-01T00:00:00.000000000','2004-01-01T00:00:00.000000000')).time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_Rez = xr.open_dataset(\"localdata/final_label_Full_Rez.nc\")\n",
    "xr_array_score = xr.DataArray(baseline_model_generator.full_grid_inf, \n",
    "                              dims=[\"time\", \"y\", \"x\"],\n",
    "                              coords={\"time\": time_slice, \n",
    "                                      \"x\": baseline_model_generator.labels.x, \n",
    "                                      \"y\": baseline_model_generator.labels.y},\n",
    "                              name=\"M1_score\")\n",
    "\n",
    "xr_array_score = xr_array_score.astype('float32')\n",
    "\n",
    "xr_array_score.to_netcdf('localdata/Model1_score_ERA5_Rez_inf.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ERA5_Rez_inf = xr.open_dataset('localdata/Model1_score_ERA5_Rez_inf.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the M1 score Xarray at Full Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_interp = xr_ERA5_Rez_inf['M1_score'].interp(x=Full_Rez.x, y=Full_Rez.y, method='nearest')\n",
    "\n",
    "fill_value = small_interp.mean().item()\n",
    "small_interp = small_interp.fillna(fill_value)\n",
    "\n",
    "xr_array_score = xr.DataArray(small_interp, \n",
    "                              dims=[\"time\", \"y\", \"x\"],\n",
    "                              coords={\"time\": time_slice, \n",
    "                                      \"x\": Full_Rez.x, \n",
    "                                      \"y\": Full_Rez.y},\n",
    "                              name=\"M1_score\")\n",
    "\n",
    "xr_array_score = xr_array_score.astype('float32')\n",
    "\n",
    "xr_array_score.to_netcdf('localdata/Model1_Score_Full_Rez_inf.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_score_ERA5_Rez_inf = xr.open_dataset('localdata/Model1_score_ERA5_Rez_inf.nc')\n",
    "np.unique(Model1_score_ERA5_Rez_inf.M1_score.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_Score_Full_Rez_inf = xr.open_dataset('localdata/Model1_Score_Full_Rez_inf.nc')\n",
    "np.unique(Model1_Score_Full_Rez_inf.M1_score.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_geo_env_dav_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
